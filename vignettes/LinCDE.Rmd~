---
title: "LinCDE vignette"
author: 
  - name: Zijun Gao
    affiliation: Department of Statistics, Stanford University
  - name: Trevor Hastie
    affiliation: Department of Statistics and Department of Biomedical Data Science, Stanford University
date: "<small>`r Sys.Date()`</small>"
output:
  html_document:
    toc: no
    toc_depth: 3
    number_sections: yes
    toc_float:
      collapsed: no
    code_folding: show
    theme: cerulean
vignette: >
  %\VignetteIndexEntry{LinCDE vignette} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteEncoding{UTF-8}{inputenc}
---

```{r global setting, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy = FALSE
)
```

In this vignette, we will introduce how to use the R package LinCDE for conditional density estimation. 

# Conditional density estimation and LinCDE

The density of a continuous random variable characterizes its relative likelihood of taking a specific value. In statistics, many questions are essentially questions of density characteristics, e.g., location, variance, modality, and skewness. A fruitful collection of methods have been proposed for density estimation, such as kernel density estimation, Lindsey's method.

In practice, the density of interest is often heterogeneous across observations. For instance, the height goes up in the mean from children to adults, the density of food expenditure is more variant among the higher-income community relative to the lower-income community, and the density of salary is bi-modal among lawyers while unimodal among firefighters. The heterogeneity in conditional densities often carries meaningful messages. 

We propose LinCDE boost to estimate the conditional densities: a boosting algorithm based on LinCDE trees. A LinCDE tree partitions the covariate space into subregions with approximately locally homogeneous densities, employs Lindsey's method for density estimation in the subregions, and aggregates the densities from different sub-areas as the estimated conditional density. LinCDE boost grows a number of LinCDE trees in a stagewise forward manner, and each tree fits the residuals of the previous estimate. For more details, please refer to our [LinCDE paper](https://jmlr.org/papers/v23/21-0840.html).

# A toy example

We illustrate the workflow of the LinCDE package via a toy example. Before all, let us install and attach the LinCDE package. 
```{r setup}
library("LinCDE")
```

The toy conditional density is locally Gaussian. We consider $20$ covariates $X = [X_1, \ldots, X_{20}]^\top$. Given a covariate value, the response $y$ is Gaussian with mean $0.5 X_1 + X_1 X_2$ and standard deviation $0.5 + 0.25 X_2$.

```{r ture density}
# locally Gaussian design (LGD)
  # If y = NULL, density.LGD generates responses y given covariates X.
  # If y != NULL, density.LGD outputs conditional densities f(y | X).
density.LGD = function(X, y = NULL){
  if(is.null(dim(X))){X = matrix(X, nrow = 1)}
  if(!is.null(y)){
    dens = dnorm(y, mean = (0.5 * X[, 1] + X[, 1] * X[, 2]), sd = (0.5 + 0.25 * X[, 2]))
  }else{
    y = 0.5 * X[, 1] + X[, 1] * X[, 2] + rnorm(dim(X)[1], 0, 1) * (0.5 + 0.25 * X[ ,2])
  }
}
```

We conduct conditional density estimation based on $1000$ training data points. Covariates $X_1$, $\ldots$, $X_{20}$ are generated independently uniformly from $[-1, 1]$. We also generate independent validation and test samples of size $1000$ for hyper-parameter tuning and performance evaluation, respectively.

```{r data}
set.seed(100)
# training data
n = 1000; d = 20; X = matrix(runif(n * d, -1, 1), ncol = d); colnames(X) = paste("X", seq(1,d), sep = "")
y.LGD = density.LGD(X)
# validation data
nVal = 1000; XVal = matrix(runif(nVal * d, -1, 1), ncol = d); colnames(XVal) = paste("X", seq(1,d), sep = "")
yVal.LGD = density.LGD(XVal)
# test data
nTest = 1000; XTest = matrix(runif(nTest * d, -1, 1), ncol = d); colnames(XTest) = paste("X", seq(1,d), sep = "")
yTest.LGD = density.LGD(XTest)
```

The lattice plot below visualizes the true conditional densities at $9$ landmark covariate values. According to the data generation mechanism, $X_1$ and $X_2$ influence the response's mean, $X_2$ also influences the response's variance, and other covariates are nuisance to the response distribution. The landmark covariates differ in $X_1$ and $X_2$. 

```{r true density plot, out.width="60%", fig.align="center", eval = F}
# landmarks for visualization
XProfile = matrix(0, nrow = 3^2, ncol = d); colnames(XProfile) = paste("X", seq(1,d), sep = "")
X1 = X2 = c(-0.6,0,0.6); XProfile[, c(1,2)] = as.matrix(expand.grid(X1, X2))

# true conditional density plot
densityPlot(X = XProfile, trueDensity = density.LGD, minY = -3, maxY = 3, factor.levels = paste("X1=", XProfile[,1], ", X2=", XProfile[,2],  sep = ""))
```

```{r true density plot (hidden), out.width="60%", fig.align="center", echo = F}
knitr::include_graphics("./figures/LGDTrueDensityPlot.png", error = FALSE)
```

## Tuning

There are a few hyper-parameters critical to the performance of LinCDE boost. We tune the hyper-parameters on a separate validation dataset.

### Primary parameters

The two primary parameters of interest are the number of iterations (_n.trees_) and the depth of the base learner LinCDE tree (_depth_).

  * _n.trees_: the number of LinCDE trees to fit. We train a LinCDE boost model on the training data with a large number of trees, e.g., 100 trees. We then compute the validation log-likelihoods after each iteration on a separate validation dataset. We choose the number of iterations corresponding to the maximal validation log-likelihood. Note that if after a number of iterations, additional trees stop contributing to the log-likelihood effectively, then we stop the boosting procedure before _n.trees_ base learners are added.

    In the following example with _depth = 2_ (parameter _depth_ discussed below), we specify the maximal number of trees as $100$. We evaluate the validation log-likelihood and plot the curve. Here we use the function _predict_ for computing the validation errors and we will discuss more in Section 2.3. The log-likelihood keeps increasing and we choose _n.trees = 100_ for _depth = 2_.

```{r depth, fig.height=6, fig.width = 6, fig.align="center", eval = FALSE}
# depth = 1
model.LGD.D1 = LinCDE.boost(X = X, y = y.LGD, depth = 1, n.trees = 100, verbose = TRUE)
predict.LGD.D1 = predict(model.LGD.D1, X = XVal, y = yVal.LGD, densityOnly = FALSE)
# depth = 2
model.LGD.D2 = LinCDE.boost(y = y.LGD, X = X, depth = 2, n.trees = 100, verbose = TRUE) 
predict.LGD.D2 = predict(model.LGD.D2, X = XVal, y = yVal.LGD, densityOnly = FALSE)
```


```{r depth (hidden), echo = FALSE, out.width= "60%", fig.align="center"}
knitr::include_graphics("./figures/LGDLoglikelihoodPlot.png", error = FALSE)
```

  * _depth_: the number of splits of each LinCDE tree. We apply LinCDE boost with a grid of depths. For each depth, we choose the number of iterations as above and record the associated validation log-likelihood. We choose the depth with the maximal validation log-likelihood. 
  
    In the above example, we experiment with _depth = 1_ and _depth = 2_. For _depth = 1_, we choose _100_ iterations, and the associated log-likelihood is $-0.80$. For _depth = 2_, we choose _100_ iterations, and the associated log-likelihood is $-0.76$. Since $-0.76 > -0.80$, we end up with _depth = 2_. Notice that the example's conditional mean involves an interaction term of $X_1$ and $X_2$, and thus _depth = 1_ --- an additive model in the density's exponent --- is too restrictive compared to _depth = 2_ --- a richer model including first-order interactions.
  
    We remark that in standard boosting, deep trees are problematic due to overfitting. In LinCDE boost, the overfitting issue is more severe than standard boosting because the density estimation problem at terminal nodes is more complicated than regression and classification. As a result, we do not recommend _depth > 5_.
    
### Secondary parameters

There are a number of secondary hyper-parameters. We recommend starting with the default values and making changes only if certain issues are observed.

  * _basis_: the type of spline basis. Default is the cubic natural spline basis. Cubic natural splines are desirable for their flexibility, smoothness, and linear extensions beyond the boundaries. However, if the conditional densities are believed to belong to a certain distribution family, then specific _basis_ should be adopted. Here are two examples. 
    + In the above example where the response is locally Gaussian, _basis = "Gaussian"_ is the most appropriate choice.
    + For the modality and skewness examples below, the response can be locally bi-modal. _basis = "Gaussian"_ can not produce bi-modality structures and a more flexible spline basis should be adopted.
    
  * _splineDf_: the number of spline basis. Default is $10$. If you go with the natural cubic spline basis, then _splineDf_ specifies the splines' degrees of freedom, i.e., the number of spline bases. A larger _splineDf_ is able to characterize more local structures but may produce unnecessary curvatures. 

  * _df_: the ridge Poisson regression's degrees of freedom. Default is $2$. _df_ is used for determining the ridge regularization hyper-parameter. A smaller _df_ corresponds to a larger regularization parameter, and assists to avoid computational instabilities at subregions with a limited number of observations. 

  * _prior_: type of the initial carrying density. Default is "Gaussian", i.e., the Gaussian distribution with the marginal response mean and the standard deviation is used as the universal density initialization. If you set _prior = "uniform"_, the uniform distribution over the response range is used. If you set _prior = "LindseyMarginal"_, the marginal response density estimated by Lindsey's method based on all responses is used. You can also input a homogeneous or heterogeneous conditional density function. The conditional density function should take a covariate matrix $X$, a response vector $y$, and output the densities at pairs $(X, y)$. If the prior conditional density is close to the underlying truth, e.g., a pretrained conditional density estimator, LinCDE boost will require less iterations. 

### Standard boosting parameters

There are several hyper-parameters applicable to all boosting-based methods. We recommend starting with the default values and making changes only if certain issues are observed.

  * _splitPoint_: a list of candidate splits or numbers of candidate split. Each element is a vector corresponding to a variable's candidate splits (including the left and right endpoints). The list's elements are ordered the same as $X$'s columns. An alternative input is candidate split numbers, a scalar if all variables share the same number of candidate splits, a vector of length nvars if variables have different numbers of candidate splits. If candidate split numbers are given, each variable's range is divided into _splitPoint-1_ intervals, i.e., _splitPoint_ knots, containing approximately the same number of observations. Default is $20$. Note that if a variable has fewer unique values than the desired number of intervals, split intervals corresponding to each unique value are created. 
  * _shrinkage_: the shrinkage parameter applied to each tree in the expansion, value in $(0,1]$. Default is $0.1$. A smaller _shrinkage_ leads to less overfitting but slower convergence.
  * _terminalSize_: the minimum number of observations in a terminal node. Default is $20$. We do not recommend a very small _terminalSize_, since fitting a density model at each terminal node may involve quite a few parameters and a reasonable number of samples are needed.
    
### Centering parameters

We will discuss the centering for LinCDE boost below, which aims to solve the "disjoint support" issue. Here we list the hyper-parameters used by the centering.

  * _centering_: If true, a conditional mean model is fitted first, and LinCDE boost is applied to the residuals. The centering is recommended for responses whose conditional support varies wildly. See below for an example. Default is false.
  * _centeringMethod_: a conditional mean estimator. Applies only when _centering_ is true. If _centeringMethod = "linearRegression"_, a regression model is fitted to the response. If _centeringMethod = "randomForest"_, a random forest model is fitted. Default is fitting a random forest. Applies only to _centering = TRUE_. If _centeringMethod_ is a function, the call _centeringMethod(y, X)_ should return a conditional mean model with a predict function. Default is "randomForest". 

## Estimation

In the above example, we tune all the primary and part of the secondary parameters on the separate validation dataset. We choose hyper-parameters _splineDf = "Gaussian"_, _shrinkage = 0.02_, _depth = 3_, _n.trees = 300_, and leave other parameters at default values. 

We print the trained LinCDE model which gives us the model formula, crucial hyper-parameters, and relative influences/importance scores (discussed in Section 2.5). Note that the boosting procedure is stopped early after $216 < 300$ iterations.

```{r LinCDE boost model, eval = FALSE}
# tuned LinCDE boost model
model.LGD.final = LinCDE.boost(y = y.LGD, X = X, basis = "Gaussian", depth = 3, n.trees = 300, shrinkage = 0.02)
print(model.LGD.final)
```

```{r LinCDE boost model (hidden), echo = FALSE}
(model.LGD.final.print = readRDS(file = "./data/modelLGDFinalPrint.rds"))
```


## Prediction

With a LinCDE boost model, we can predict conditional densities of an independent test dataset via function _predict_.
```{r test performance 3, eval = FALSE}
predict.LGD.final.simple = predict(model.LGD.final, X = XTest, y = yTest.LGD)
```

We can also use _predict_ to produce conditional density curves. In this case, we input test covariates and no test responses are needed. By default, _predict_ computes conditional densities at response break points used in training. We also allow users to input handcrafted break points through _splitPointYTest_.
```{r test performance 4, eval = FALSE}
predict.LGD.final.density = predict(model.LGD.final, X = XTest)
```

Finally, it is possible to use _predict_ to achieve both targets: conditional density predictions at test covariate-response pairs and conditional density curves at test covariates. Simply set _densityOnly = FALSE_ in _predict_. Check out the documentary of _predict_ for the full list of returned values.
```{r test performance 2, eval = FALSE}
predict.LGD.final = predict(model.LGD.final, X = XTest, y = yTest.LGD, densityOnly = FALSE)
```

We evaluate the performance of LinCDE boost. On simulated data, we adopt the metric: the relative improvement in the test log-likelihood
\begin{align*}
  \frac{\ell_{\text{LinCDE boost}} - \ell_{\text{null}}}{\ell_{\text{oracle}} - \ell_{\text{null}}},
\end{align*}
where the null model is the universal Gaussian distribution with the response's marginal mean and standard deviation, and the oracle denotes the true underlying conditional density. The criterion is analogous to the goodness-of-fit measure $R^2$ of linear regression. In this example, LinCDE boost achieves $80.9\%$ of the oracle's improvement over the null model.

```{r test performance, eval = FALSE}
# null model
model.LGD.null = LinCDE.boost(X = X, y = y.LGD, basis = "Gaussian", n.trees = 0)
predict.LGD.null = predict(model.LGD.null, X = XTest, y = yTest.LGD,  densityOnly = FALSE)
# oracle
oracle = mean(log(density.LGD(X = XTest, y = yTest.LGD)))
# relative improvement 
(relativeImprovement =   
  (predict.LGD.final$testLogLikelihood - predict.LGD.null$testLogLikelihood)/  
  (oracle - predict.LGD.null$testLogLikelihood))
```

```{r test performance (hidden), echo = FALSE}
predict.LGD.null.short = readRDS(file = "./data/predictLGDNullShort.rds")
predict.LGD.final.short = readRDS(file = "./data/predictLGDFinalShort.rds")

oracle = mean(log(density.LGD(X = XTest, y = yTest.LGD)))
relativeImprovement =   
  (predict.LGD.final.short$testLogLikelihood - predict.LGD.null.short$testLogLikelihood)/  
  (oracle - predict.LGD.null.short$testLogLikelihood) # 80.9%; 83.2%
print(paste("relative improvement: ", signif(100 * relativeImprovement, digits = 3), "%", sep = ""))
```

When the true density is unknown, such as on a real dataset, the relative improvement can't be computed. However, we can still obtain and use the test log-likelihood as an assessment of LinCDE boost's performance. 

## Visualization

We plot the estimated conditional densities against the truth at the above $9$ selected feature points. The estimated conditional densities are close to the truth.

```{r estimated density plot, out.width = "60%", fig.align="center", eval = F}
# conditional density plot
densityPlot(X = XProfile, trueDensity = density.LGD, model = model.LGD.final, factor.levels = paste("X1=", XProfile[,1], ", X2=", XProfile[,2],  sep = ""))
```

```{r estimated density plot (hidden), out.width = "60%", fig.align="center", echo = F}
knitr::include_graphics("./figures/LGDFinalDensityPlot.png", error = FALSE)
```

## Importance evaluation

To identify the influential covariates, we plot the importance scores for each covariate. The importance score is defined as in random forests and graient boosting. The importance scores are computed from the training data and record how much each covariate contributes to the improvement of the objective.

In the above example, the importance score barplot indicates that the first two candidates contribute the most to the improvement in the objective, which agrees with the underlying density model.

```{r importance plot LGD, out.width = "60%", fig.align="center", eval = F}
# relative influence plot
importance.LGD = summary(model.LGD.final, cBars = 8)
```

```{r importance plot LGD (hidden), out.width = "60%", fig.align="center", echo = F}
knitr::include_graphics("./figures/LGDFinalImportancePlot.png", error = FALSE)
```

# More examples
The above example shows LinCDE boost's ability to capture location and scale changes. We add two more examples focusing on modality and skewness (asymmetry), respectively. We also include an example with the "disjoint support" issue.

## Modality example

For modality, we generate locally Gaussian mixture responses if $X_2 \le 0.2$ and locally Gaussian responses if $X_2 > 0.2$. Meanwhile, we let the responses' locations depend on $X_1$. We follow the aforementioned workflow and set the hyper-parameters at _shrinkage = 0.05_, _depth = 3_, _n.trees = 200_. We use a larger _df_ to learn the bi-modality. The rest of the parameters are at default values.

```{r modality example, eval=FALSE}
# data generation mechanism
density.modality = function(X, y = NULL){
  if(is.null(dim(X))){X = matrix(X, nrow = 1)}
  if(!is.null(y)){
    dens = dnorm(y, mean = 0.25 * X[,1], sd = 0.8)
    index = which(X[,2] < 0.2)
    dens[index] = 0.5 * dnorm(y[index], mean = 0.25 * X[index, 1] + 0.8, sd = 0.3) + 0.5 * dnorm(y[index], mean = 0.25 * X[index, 1] - 0.8, sd = 0.3)
    return(dens)
  }else{
    n = dim(X)[1]
    groupIndex = rbinom(n, 1, 0.5)
    y = groupIndex * rnorm(n, -0.8, 0.3) + (1-groupIndex) * rnorm(n, 0.8, 0.3)
    y = 0.25 * X[,1] + (X[,2] <= 0.2) * y + (X[,2] > 0.2) * rnorm(n, 0, 0.8)
  }
}
set.seed(1)
y.modality = density.modality(X)
model.modality = LinCDE.boost(y = y.modality, X = X, df = 8,
                                 depth = 3, n.trees = 200, shrinkage = 0.05)
```

We compare the estimated conditional densities against the truth at the $9$ selected feature points (left figure below). The estimated conditional densities are clearly bi-modal for $X_2 = -0.6$ and $X_2 = 0$. For $X_2 = 0.6$, the estimated conditional densities are largely Gaussian with mild curvatures in the middle.

We also plot the importance scores (right figure below). In this setting, $X_1$ and $X_2$ are the most influential covariates, consistent with the data generation mechanism.

```{r modality plot,fig.height=6, fig.width = 6, fig.show='hold', out.width="50%", echo = F}
knitr::include_graphics("./figures/modalityDensityPlot.png", error = FALSE)
knitr::include_graphics("./figures/modalityImportancePlot.png", error = FALSE)
```

## Symmetry example

For symmetry, we generate asymmetric Gaussian mixture responses. If $X_2 > 0$, the right modal is sharper; if $X_2 < 0$, the left modal is sharper. The larger the $|X_2|$ is, the more asymmetric the conditional distribution is. Meanwhile, we let the responses' locations depend on $X_1$. We follow the aforementioned workflow and set the hyper-parameters at _shrinkage = 0.1_, _depth = 3_, _n.trees = 200_, and _df = 8_. The rest parameters are at default values.

```{r skewness example, eval = FALSE}
# data generation mechanism
density.skewness = function(X, y = NULL){
  if(is.null(dim(X))){X = matrix(X, nrow = 1)}
  if(!is.null(y)){
    dens = 0.5 * dnorm(y, mean = 0.5 * X[, 1] + 0.8, sd = 0.5 + 0.45 * X[, 2]) +
    0.5 * dnorm(y, mean = 0.5 * X[, 1] - 0.8, sd = 0.5 - 0.45 * X[, 2])
  }else{
    n = dim(X)[1]
    groupIndex = rbinom(n, 1, 0.5)
    y = groupIndex * rnorm(n, 0.8, 0.5 + 0.45 * X[, 2]) + (1 - groupIndex) * rnorm(n, -0.8, 0.5 - 0.45 * X[, 2]) + 0.5 * X[, 1]
  }
}

set.seed(1)
y.skewness = density.skewness(X)
model.skewness = LinCDE.boost(X = X, y = y.skewness, df = 8,
                                 depth = 3, n.trees = 200, shrinkage = 0.1)
```

We compare the estimated conditional densities against the truth at $9$ landmarks (left figure below). The estimated conditional densities are right-skewed for $X_2 = 0.6$, left-skewed for $X_2 = -0.6$, and symmetric for $X_2 = 0$.

As for the importance scores (right figure below), $X_1$ and $X_2$ are correctly identified as important covariates.

```{r skewness plot, fig.height=6, fig.width = 6, fig.show='hold', out.width="50%", echo = F}
knitr::include_graphics("./figures/skewnessDensityPlot.png", error = FALSE)
knitr::include_graphics("./figures/skewnessImportancePlot.png", error = FALSE)
```

## Centering example

Centering aims to align the centers of the conditional densities in advance by estimating the conditional means first and subtracting the estimates from the responses. Then we apply LinCDE boost to the residuals to capture additional distributional structures. The centering step is helpful if a distribution's conditional components differ violently in location, i.e., the "disjoint support" issue. For more details regarding LinCDE boost and centering, please refer to our [LinCDE paper](https://jmlr.org/papers/v23/21-0840.html).
In practice, we recommend creating scatter plots of responses versus (a selected subset of) covariates to detect the "disjoint support" issue.

In the following example, we generate locally Gaussian mixture responses if $X_2 \le 0.2$, and locally Gaussian responses if $X_2 > 0.2$. Meanwhile, we let the responses' supports differ dramatically as $X_1$ changes, and thus the "disjoint support" problem is present.

```{r centering model, eval = F}
# data generation mechanism
density.centering = function(X, y = NULL){
  if(is.null(dim(X))){X = matrix(X, nrow = 1)}
  if(!is.null(y)){
    dens = dnorm(y, mean = 4 * X[, 1], sd = 0.8)
    index = which(X[, 2] < 0.2)
    dens[index] = (0.5 * dnorm(y[index], mean = 4 * X[index, 1] + 0.8, sd = 0.3) + 0.5 * dnorm(y[index], mean = 4 * X[index, 1] - 0.8, sd = 0.3))
    return(dens)
  }else{
    n = dim(X)[1]
    groupIndex = rbinom(n, 1, 0.5)
    y = groupIndex * rnorm(n, -0.8, 0.3) + (1 - groupIndex) * rnorm(n, 0.8, 0.3)
    y = 4 * X[, 1] + (X[, 2] <= 0.2) * y + (X[, 2] > 0.2) * rnorm(n, 0, 0.8)
  }
}

# generate data
set.seed(1)
y.centering = density.centering(X)
```

We follow the aforementioned workflow. For LinCDE boost without centering, we set the hyper-parameters at _shrinkage = 0.02_, _depth = 3_, _n.trees = 200_ (the rest parameters are at default values). For LinCDE boost with centering, we use linear regression as the centering method by setting _centeringMethod = "linearRegression"_. Check out the documentary of _LinCDE.boost_ for possible centering methods.
```{r centering model 2, eval = F}
# without centering
model.centering.no = LinCDE.boost(X = X,  y = y.centering, depth = 2, n.trees = 200, shrinkage = 0.02, splineDf = 10)
# with centering
model.centering.OLS = LinCDE.boost(X = X, y = y.centering, df = 8, depth = 2, n.trees = 200, shrinkage = 0.02, centering = TRUE, centeringMethod = "linearRegression")
```

We compare LinCDE boost with and without centering. We plot the estimated conditional densities at $9$ landmark covariate values.
LinCDE boost without centering does not capture the bi-modality structure. LinCDE boost with centering reflects the bi-modality structure for $X_2 = 0$ or $X_2 = -0.6$.

```{r centering density plot, fig.show="hold", out.width="50%", fig.height=6, fig.width = 6, echo = F}
knitr::include_graphics("./figures/centeringNoDensityPlot.png", error = FALSE)
knitr::include_graphics("./figures/centeringOLSDensityPlot.png", error = FALSE)
```

We also compare the relative influence plots. For LinCDE boost without centering, most efforts are spent on learning the influence of $X_1$ on the response's location. For LinCDE boost with centering, we stack importances from centering (in red) and beyond centering (in blue). $X_1$ accounts for most of the importances in the centering, and $X_2$ is the most important covariate beyond centering. Currently the decomposition of importances into centering and beyond centering is only available for _centeringMethod = "linearRegression"_ or _centeringMethod = "randomForest"_.

```{r without centering density plot, fig.show="hold", out.width="50%", fig.height=6, fig.width = 6, echo = F}
knitr::include_graphics("./figures/centeringNoImportancePlot.png", error = FALSE)
knitr::include_graphics("./figures/centeringOLSImportancePlot.png", error = FALSE)
```

## California housing example

We read in the benchmark dataset California Housing data. The dataset collects California housing prices and $8$ covariates from the $1990$ Census. More details are available at the [website](https://developers.google.com/machine-learning/crash-course/california-housing-data-description). We remove windsorized responses, and divide the responses by $1000$. For time efficiency, we randomly select $1000$ observations as the training data.
```{r CA housing data}
cahousing=read.csv("./data/cahousing.csv", header = T)
y.cahousing = cahousing[,1]; cahousing=cahousing[y.cahousing < max(y.cahousing),]
y.cahousing = cahousing[,1]/1000
X = data.matrix(cahousing[,-1])
set.seed(318); indexTrain = sample(length(y.cahousing), 1000, replace = FALSE)
```

```{r CA housing data (hidden), echo = F}
X.transform = X
X.transform[,c("MedInc", "AveRooms", "AveBedrms", "Population", "AveOccup")] = log(X[,c("MedInc", "AveRooms", "AveBedrms", "Population", "AveOccup")])
```

We fit a simple LinCDE boost model with sufficient statistics $y$,$y^2$ as a start. The LinCDE boost model incorporates the information of location and variance. The resulting conditional densities are locally Gaussian.

```{r Gaussian fit, eval = F}
model.cahousing.Gaussian = LinCDE.boost(y = y.cahousing[indexTrain], X=X[indexTrain,], basis = "Gaussian", depth=3, n.trees=150, terminalSize=50, shrinkage = 0.05, minY = 0, maxY = 510)
```

Next we use $10$ transformed natural cubic splines with $6$ degrees of freedom to learn a more complicated LinCDE boost model.

```{r ns fit, eval = F}
model.cahousing.ns = LinCDE.boost(y=y.cahousing[indexTrain],X=X[indexTrain,], basis = "nsTransform", depth=3, splineDf = 10, df = 6, n.trees=300, terminalSize=50, shrinkage = 0.05, minY = 0, maxY = 510, verbose = TRUE)
```

We provide the relative influence plots. The importances computed by the two LinCDE boost models are similar: *MedInc* (median income) is the dominant covariate followed by *AveOccup* (average occupancy), *Longitude*, and *Latitude*.

```{r importance plot CA housing, fig.show="hold", out.width="50%", echo = F}
knitr::include_graphics("./figures/cahousingGaussianImportancePlot.png", error = FALSE)
knitr::include_graphics("./figures/cahousingNsImportancePlot.png", error = FALSE)
```

We plot the conditional density estimates for different (*MedInc*, *AveOccup*) pairs. Since we don't know the underlying truth, we plot the histograms of the samples with similar covariate values as our reference. (The similarity of samples to a target point is measured by the covariate Mahalanobis distance weighted by the covariate importances from the LinCDE boost model.)

For both LinCDE boost models, the conditional densities shift to the right as the *MedInc* increases, and the spreads decrease as the *AveOccup* grows in the middle and right columns. For the LinCDE boost model with natural spline basis, the estimated conditional densities have diverse shapes beyond Gaussian. In local regions (indicated in the subtitles of panels) of $2.32$ ($20\%$ quantile) or $3.45$ ($50\%$ quantile) median incomes, the conditional distributions appear right-skewed. In regions of $3.45$ ($50\%$ quantile) median income and $2.36$ ($20\%$ quantile), $2.84$ ($50\%$ quantile) average occupancies, the conditional distributions have flatter peaks compared to the Gaussian distribution.

```{r Gaussian plot, fig.show="hold", out.width="50%", fig.height=5, fig.width = 5, eval = F}
# landmarks for visualization
X.MedInc = quantile(X[,"MedInc"],c(.2,.5,.8)) # 20%, 50%, 80% quantiles of MedInc
X.AveOccup = quantile(X[,"AveOccup"],c(.2,.5,.8))  # 20%, 50%, 80% quantiles of AveOccup
XGrid=matrix(colMeans(X), byrow=TRUE, nrow=9, ncol=ncol(X)); colnames(XGrid) = colnames(X) # other covariates fixed at the sample means
XGrid[,c("MedInc", "AveOccup")] = data.matrix(expand.grid(X.MedInc, X.AveOccup))
yGrid = seq(0,510,length.out = 100) # response grid

# conditional density plot
# Gaussian basis
plot.cahousing.Gaussian = densityPlot(X = XGrid, yGrid = yGrid, model=model.cahousing.Gaussian, plot = FALSE)
# natural spline basis
plot.cahousing.ns = densityPlot(X=XGrid, yGrid = yGrid, model = model.cahousing.ns, plot = FALSE)
```

```{r Gaussian plot 2, fig.show="hold", out.width="50%", fig.height=8, fig.width = 8, echo = F}
knitr::include_graphics("./figures/cahousingGaussianDensityPlot.png", error = FALSE)
knitr::include_graphics("./figures/cahousingNsDensityPlot.png", error = FALSE)
```

## Plug in your own data and have fun with LinCDE!

